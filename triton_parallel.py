import asyncio
from fastapi import FastAPI
from pydantic import BaseModel
import tritonclient.grpc.aio as grpcclient
import numpy as np
import json
import uvicorn
import uuid


app = FastAPI()

# Triton client
triton_client: grpcclient.InferenceServerClient = None

# ë¹„ë™ê¸° ìš”ì²­ í
request_queue = asyncio.Queue()

# ìš”ì²­ ë°ì´í„° êµ¬ì¡°
class Request(BaseModel):
    prompt: str

class InferenceTask:
    def __init__(self, prompt: str):
        self.id = str(uuid.uuid4())
        self.prompt = prompt
        self.future = asyncio.get_event_loop().create_future()

# FastAPI ìš”ì²­ ì²˜ë¦¬
@app.post("/generate/")
async def generate(req: Request):
    task = InferenceTask(req.prompt)
    await request_queue.put(task)
    result = await task.future  # ì‘ë‹µ ì˜¬ ë•Œê¹Œì§€ ëŒ€ê¸°
    return {"response": result}

# Worker: íì—ì„œ ì—¬ëŸ¬ ìš”ì²­ ëª¨ì•„ì„œ Tritonì— stream_infer()
async def stream_worker():


    while True:
        batch = []
        # 50ms ë™ì•ˆ ëª¨ì•„ì„œ ë°°ì¹˜ êµ¬ì„±
        try:
            while len(batch) < 4:  # ìµœëŒ€ batch size
                task = await asyncio.wait_for(request_queue.get(), timeout=0.05)
                batch.append(task)
        except asyncio.TimeoutError:
            if not batch:
                continue  # ì•„ë¬´ ê²ƒë„ ì—†ìŒ

        # Triton ìš”ì²­ êµ¬ì„±
        inputs_list = []
        for task in batch:
            inputs = []

            prompt_np = np.array([task.prompt.encode("utf-8")], dtype=np.object_)
            inputs.append(grpcclient.InferInput("text_input", [1], "BYTES"))
            inputs[-1].set_data_from_numpy(prompt_np)

            inputs.append(grpcclient.InferInput("stream", [1], "BOOL"))
            inputs[-1].set_data_from_numpy(np.array([True], dtype=bool))

            sampling = {"temperature": "0.1", "top_p": "0.95", "max_tokens": "100"}
            sampling_bytes = np.array([json.dumps(sampling).encode("utf-8")], dtype=np.object_)
            inputs.append(grpcclient.InferInput("sampling_parameters", [1], "BYTES"))
            inputs[-1].set_data_from_numpy(sampling_bytes)

            inputs.append(grpcclient.InferInput("exclude_input_in_output", [1], "BOOL"))
            inputs[-1].set_data_from_numpy(np.array([True], dtype=bool))

            outputs = [grpcclient.InferRequestedOutput("text_output")]

            inputs_list.append({
                "model_name": "vllm_model",
                "inputs": inputs,
                "outputs": outputs,
                "request_id": task.id,  # ê°„ë‹¨íˆ ì‹ë³„
                "parameters": sampling,
            })

        # Triton stream ìš”ì²­
        response_iterator = triton_client.stream_infer(inputs_iterator=iter(inputs_list))
        task_map = {task.id: task for task in batch}
        # breakpoint()
        print("ğŸ“¡ Triton ì‘ë‹µ ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")
        async for response in response_iterator:
            print("ğŸŸ¢ ì‘ë‹µ ìˆ˜ì‹ ë¨")
            result, error = response
            if error:
                print("Error:", error)
                continue
            text = result.as_numpy("text_output")[0].decode("utf-8")
            prompt_id = result.get_response().id
            # task ì°¾ê¸°
            # ì•ˆì „í•œ ë§¤í•‘
            task = task_map.get(prompt_id)
            if task:
                task.future.set_result(text)
            else:
                print(f"âš ï¸ Unknown response ID: {prompt_id}")

# FastAPI ì„œë²„ ì‹œì‘ ì‹œ worker ì‹¤í–‰
@app.on_event("startup")
async def startup_event():
    global triton_client
    # Triton clientë¥¼ í˜„ì¬ ë£¨í”„ì—ì„œ ìƒì„±
    triton_client = grpcclient.InferenceServerClient("localhost:8001")
    asyncio.create_task(stream_worker())

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8003)
